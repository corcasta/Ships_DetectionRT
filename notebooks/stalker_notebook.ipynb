{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40c0c4c3-82e0-4df1-bc00-f5eb2fda6819",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from glob import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import imutils\n",
    "import supervision as sv\n",
    "import time\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16032243-d013-40e3-832b-d0f1033f04d7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[       1368,           0,      994.07],\n",
       "       [          0,        1367,      493.42],\n",
       "       [          0,           0,           1]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cam_intrinsics = pd.read_csv(\"cam_intrinsics.txt\", delim_whitespace=True, header=None) \n",
    "cam_intrinsics = cam_intrinsics.to_numpy()\n",
    "cam_intrinsics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "470c1a89-bf83-4fd8-aa58-ac1b86fb888c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def denormalize(points, camera_resolution=(1920, 1080)):\n",
    "    \"\"\"\n",
    "    Normalization Formula,\n",
    "\n",
    "    Normalized(Xcentre) = (x_centre)/Image_Width\n",
    "\n",
    "    Normalized(Ycentre) = (y_centre)/Image_Height\n",
    "\n",
    "    Normalized(w) = w/Image_Width\n",
    "\n",
    "    Normalized(h) = h/Image_Height\n",
    "    \n",
    "          # Normalization\n",
    "      x_centre = x_centre / img_w\n",
    "      y_centre = y_centre / img_h\n",
    "      w = w / img_w\n",
    "      h = h / img_h\n",
    "    \"\"\"\n",
    "    points[:, 0] = points[:, 0]*camera_resolution[0]\n",
    "    points[:, 1] = points[:, 1]*camera_resolution[1]\n",
    "    points[:, 2] = points[:, 2]*camera_resolution[0]\n",
    "    points[:, 3] = points[:, 3]*camera_resolution[1]\n",
    "    \n",
    "    return points\n",
    "    \n",
    "    \n",
    "\n",
    "def localize(points):\n",
    "    #Drone pose relative to World frame:\n",
    "    phi   = np.deg2rad(-1)\n",
    "    theta = 0\n",
    "    psi   = 0\n",
    "    Od_x  = 0\n",
    "    Od_y  = 0\n",
    "    Od_z  = 0.23\n",
    "    \n",
    "    rot_x = [[1,           0,            0],\n",
    "             [0, np.cos(phi), -np.sin(phi)],\n",
    "             [0, np.sin(phi),  np.cos(phi)]]\n",
    "    rot_x = np.asarray(rot_x)\n",
    "    \n",
    "    rot_y = [[np.cos(theta),  0, np.sin(theta)],\n",
    "             [0,              1,             0],\n",
    "             [-np.sin(theta), 0, np.cos(theta)]]\n",
    "    rot_y = np.asarray(rot_y)\n",
    "    \n",
    "    rot_z = [[np.cos(psi), -np.sin(psi), 0],\n",
    "             [np.sin(psi),  np.cos(psi), 0],\n",
    "             [0,                      0, 1]]\n",
    "    rot_z = np.asarray(rot_z)\n",
    "    \n",
    "    rot_wd = rot_x @ rot_y @ rot_z\n",
    "    t_wd = np.vstack((rot_wd, [0,0,0]))\n",
    "    t_wd = np.hstack((t_wd, [[Od_x], [Od_y], [Od_z], [1]]))\n",
    "    \n",
    "    \n",
    "    #Camera pose relative to Drone frame:\n",
    "    #For the moment we are just assuming that the camera is fixed\n",
    "    #to the drone's movement. Later we will need to provide the gimball\n",
    "    #orientation to have the correct matrix for the camera relative to \n",
    "    #the drone.\n",
    "    t_dc = [[1,  0, 0, 0],\n",
    "            [0,  0, 1, 0],\n",
    "            [0, -1, 0, 0],\n",
    "            [0,  0, 0, 1]]\n",
    "    t_dc = np.asarray(t_dc)\n",
    "    \n",
    "    #Camera pose relative to World frame:\n",
    "    t_wc = t_wd@t_dc\n",
    "    \n",
    "    \n",
    "    #Camera Intrinsics\n",
    "    cam_intrinsics = pd.read_csv(\"cam_intrinsics.txt\", delim_whitespace=True, header=None) \n",
    "    cam_intrinsics = cam_intrinsics.to_numpy()\n",
    "    \n",
    "    #Localize point in World Coords\n",
    "    points = np.vstack((points, np.ones((1, len(points[0,:])))))\n",
    "    #print(points)\n",
    "    A_cpx = np.linalg.inv(cam_intrinsics) @ points \n",
    "    #print(A_cpx)\n",
    "    R_wc = t_wc[0:3, 0:3]\n",
    "    O_wc = t_wc[0:3, 3][:, np.newaxis]\n",
    "    Z = np.asarray([[0],[0],[1]])\n",
    "    #print((-O_wc[2,:]/(np.dot(R_wc, A_cpx)[2,:])))\n",
    "    #print(np.dot(R_wc, A_cpx))\n",
    "    #print((-O_wc[2,:]/(np.dot(R_wc, A_cpx)[2,:]))*np.dot(R_wc, A_cpx))\n",
    "    #print(O_wc)\n",
    "    A_wk = O_wc + (-O_wc[2,:]/(np.dot(R_wc, A_cpx)[2,:]))*np.dot(R_wc, A_cpx)\n",
    "    \n",
    "    return A_wk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e94ba5bb-bdc2-4622-b7a6-bbcf1dc8de30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "points = np.asarray([[1026,   940.5], \n",
    "                     [941.63, 754.87]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc4378a-3cf0-4cff-924b-50fa8908bcd6",
   "metadata": {},
   "source": [
    "#Loading trained weights\n",
    "weights = sorted(glob(os.path.join(os.getcwd(), \n",
    "                                   \"runs\", \n",
    "                                   \"detect\", \n",
    "                                   \"train\", \n",
    "                                   \"weights\", \n",
    "                                   \"*.pt\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5811f138-65b5-4485-8f6c-62280b5956a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#best_weights = weights[0]\n",
    "#Model Instance\n",
    "model = YOLO(\"yolov8m.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d92a7f1e-9e6e-4c8f-a6b9-ec920dd09689",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.46 ðŸš€ Python-3.9.16 torch-1.13.1+cu117 CPU\n",
      "YOLOv8m summary (fused): 218 layers, 25886080 parameters, 0 gradients, 78.9 GFLOPs\n",
      "\n",
      "Warning: Ignoring XDG_SESSION_TYPE=wayland on Gnome. Use QT_QPA_PLATFORM=wayland to run on Wayland anyway.\n",
      "image 1/1 /home/corcasta/Ships_DetectionRT/localization/undistorted_test_measurement.jpg: 384x640 1 sports ball, 329.6ms\n",
      "Speed: 0.5ms preprocess, 329.6ms inference, 6.2ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pixel_Coords: [[  0.0035055]\n",
      " [     1.0788]\n",
      " [          0]] \n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    model = YOLO(\"yolov8m.pt\")\n",
    "    #*********************FPS-DISPLAY-SETTINGS***********************\n",
    "    # used to record the time when we processed last frame\n",
    "    prev_frame_time = 0\n",
    "    # used to record the time at which we processed current frame\n",
    "    new_frame_time = 0\n",
    "    # font which we will be using to display FPS\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    #*********************FPS-DISPLAY-SETTINGS***********************\n",
    "    \n",
    "    box_annotator = sv.BoxAnnotator(\n",
    "            thickness=1,\n",
    "            text_thickness=1,\n",
    "            text_scale=0.5,\n",
    "            text_padding=5\n",
    "        )\n",
    "\n",
    "        ##480x640\n",
    "        #frame_width = 480\n",
    "        #frame_height = 640\n",
    "        #cap = cv2.VideoCapture(video)\n",
    "        #cap.set(cv2.CAP_PROP_FRAME_WIDTH, frame_width)\n",
    "        #cap.set(cv2.CAP_PROP_FRAME_HEIGHT, frame_height)\n",
    "\n",
    "    for result in model.track(source=\"undistorted_test_measurement.jpg\", classes=32, show=False, stream=True):\n",
    "\n",
    "        frame = result.orig_img\n",
    "        detections = sv.Detections.from_yolov8(result)\n",
    "\n",
    "        if result.boxes.id is not None:\n",
    "            detections.tracker_id = result.boxes.id.cpu().numpy().astype(int) \n",
    "            \n",
    "            points = denormalize(result.boxes.xywhn, camera_resolution=(1920, 1080))\n",
    "            x_coords = points[:, 0]\n",
    "            y_coords = points[:, 1]\n",
    "            points = torch.transpose(points[:, 0:2], 0, 1).numpy()\n",
    "            points_world = localize(points)\n",
    "            print(\"Pixel_Coords: {} \".format(points_world))\n",
    "            \n",
    "            \n",
    "        labels = [\n",
    "            f\"#:{tracker_id}, {model.model.names[class_id]}: {confidence:0.2f}\"\n",
    "            for _, confidence, class_id, tracker_id \n",
    "            in detections\n",
    "        ]\n",
    "\n",
    "        frame = box_annotator.annotate(scene=frame, detections=detections, labels=labels)\n",
    "\n",
    "        # time when we finish processing for this frame\n",
    "        new_frame_time = time.time()\n",
    "        # fps will be number of frame processed in given time frame\n",
    "        # since their will be most of time error of 0.001 second\n",
    "        # we will be subtracting it to get more accurate result\n",
    "        fps = 1/(new_frame_time-prev_frame_time)\n",
    "        prev_frame_time = new_frame_time\n",
    "        # converting the fps into integer\n",
    "        fps = int(fps)\n",
    "        # converting the fps to string so that we can display it on frame\n",
    "        # by using putText function\n",
    "        fps = str(fps)\n",
    "\n",
    "        # putting the FPS count on the frame\n",
    "        cv2.putText(frame, \"FPS: {}\".format(fps), (7, 30), font, 1, (100, 255, 0), 3, cv2.LINE_AA)\n",
    "\n",
    "\n",
    "        cv2.imshow(\"Ships-Detection-YoloV8n\", frame)\n",
    "\n",
    "\n",
    "        if (cv2.waitKey(20) == ord(\"q\")):\n",
    "            break\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    main()\t\t\t\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296b1184-b042-40f8-9573-85626223e6f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf7d869-72a8-466d-a670-41b1a71613b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2defc868-82e5-4251-8cd1-b917596a37a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
